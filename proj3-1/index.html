<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
</style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
<h1 align="middle">Project 3-1: Path Tracer</h1>
<h2 align="middle">Tiger Ma & William Shue</h2>

<!-- Add Website URL -->
<h2 align="middle">Website URL: <a href="TODO">https://wshue0.github.io/chicken-prep/proj3-1/index.html</a></h2>

<br><br>


<div align="center">
  <table style="width=100%">
      <tr>
          <td align="middle">
          <img src="images/example_image.png" width="480px" />
          <figcaption align="middle">Results Caption: my bunny is the bounciest bunny</figcaption>
      </tr>
  </table>
</div>


<h2 align="middle">Overview</h2>
<p>
  In this project, we dive into rendering meshes and introducing lighting to them in order to render realistic images.
  In part 1, we generate rays from a camera and transform them into world coordinates. This allows us to sample the scene at a point
  where the ray intersects it. In part 2, we speed up this process by implementing bounding volume hierarchies. This improves
  the runtime from O(N) to O(logN) because we don't have to check all primitives for an intersection. Traversing a BVH divides
  the set of primitives that we have to check by half each time, thus giving us an O(logN) runtime. Thus, we BVH allows us
  to render more complex meshes. In part 3, we add lighting to rendering, namely direct illumination. We first implemented
  hemisphere sampling, which gave us noisy images. However, importance sampling allowed us to invert the ray to check rays only coming from light
  sources, which helped mitigate the noise. In part 4, we addressed the issue of dark unrealistic shadows by implementing
  global illumination. On top of direct illumination, we now consider the bouncing of rays, which help light up places that are
  not directly in the line of sight of light sources. In part 5, we implement adaptive sampling in order to improve efficiency.
  This is done by checking if the irradiance has converged and if so we can save work by terminating early. Overall, we learned
  how to generate realistic images with direct/indirect lighting with efficiency.
</p>
<br>

<h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
<!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

<h3>
  Walk through the ray generation and primitive intersection parts of the rendering pipeline.
</h3>
<p>
  We first transformed the given coordinates from sensor space to camera space by normalizing with the (0.5,0.5) origin of the sensor and then scaling with the size of the camera fov.
  Next, we transformed this into world coordinates my multiplying by the c2w matrix and then normalized the vector. For raytracing a single pixel, we simply took an average of the illuminance of the 
  generated rays over n_sample iterations. We then updated the pixel with this value in the sample buffer.
</p>
<br>

<h3>
  Explain the triangle intersection algorithm you implemented in your own words.
</h3>
<p>
  For the triangle intersection, we used the Moller-Trumbore formula and plugged in the respective values to obtain the barycentric coordiantes. We performed some bounds checking
  by ensuring the barycentric coordinates were between [0,1] and that t was between min_t and max_t. After this we could calculate the barycentric-weighted normal vector and set the 
  parameters in the intersection structure accordingly.
</p>
<br>

<h3>
  Show images with normal shading for a few small .dae files.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/11.png" align="middle" width="400px"/>
        <figcaption>banana.dae</figcaption>
      </td>
      <td>
        <img src="images/12.png" align="middle" width="400px"/>
        <figcaption>building.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/13.png" align="middle" width="400px"/>
        <figcaption>cow.dae</figcaption>
      </td>
      <td>
        <img src="images/14.png" align="middle" width="400px"/>
        <figcaption>teapot.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


<h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
<!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

<h3>
  Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
</h3>
<p>
  In our BVH construction algorithm we decided to use a average centroid of all primitives' bounding boxes
  as the splitting criteria. This means that for a given set of primitives, we compute the average centroid, and choose an axis. For the chosen axis, we then sort each primitive into either
  the left or right group based on whether primitive's bounding box centroid is geometrically to the left or right of the average centroid in that particular axis. For choosing an axis, we 
  precompute the split ratio for each of the three axes and then choose the one with the most even split (whichever maximizes the function 1*(1-x) where x is the proportion in left group). Thus, we first counted the total number of primitives and computed an average centroid as well.
  Next, using this splitting criteria we used vector::partition to partition the list of primitives in-place (so that recursive calls will all access the same values in memory)
  into the left and right groups. When we make recursive calls in the non-leaf case, we pass on a modified both a start and end iterator for each of the left and right groups. 
  In the leaf node case, we simply return a node that contains an iterator with the list of primitives. The leaf-node stopping condition is a max_leaf_number, so we create a leaf 
  node when the remaining list of primitives is of size max_leaf_number or less.
</p>

<h3>
  Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/21.png" align="middle" width="400px"/>
        <figcaption>beast.dae</figcaption>
      </td>
      <td>
        <img src="images/22.png" align="middle" width="400px"/>
        <figcaption>peter.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/23.png" align="middle" width="400px"/>
        <figcaption>beetle.dae</figcaption>
      </td>
      <td>
        <img src="images/24.png" align="middle" width="400px"/>
        <figcaption>dragon.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.
</h3>
<p>
  With BVH:
  cow.dae 0.0223s (5856 prims)
  beetle.dae 0.0223s (7558 prims)
  peter.dae 0.0778s (40018 prims)
</p>
<p>
  Without BVH:
  cow.dae 20.9353s
  beetle.dae 26.9263s
  peter.dae 149.3365
</p>
<p>
  Basically, there was a huge speed up with BVH acceleration that was well on the order of 1000x. When taking the number of primitives into account for each scene, the speed up also 
  shows a speed up from O(N) to O(log(n)). This happens because of the hierarchical structure of BVH that enables us to
  skip over a vast majority of the primitives and save time computing unncessary ray-primitive intersections. Additionally, most of these scenes were centralized, or had well defined planes of symmetry, which 
  benefits our heuristic of splitting along axes, since roughly each split would have an even split, allowing the BVH to be more balanced and the height to be decreased.
</p>
<br>

<h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
<!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

<h3>
  Walk through both implementations of the direct lighting function.
</h3>
<p>
  We implemented two methods for direct lighting: uniform hemisphere sampling and importance sampling. For uniform hemisphere sampling,
  we uniformly sample a solid angle in the hemisphere and get the radiance, which is derived using the Monte Carlo estimator
  as bdsf(p, wi->wo) * L(p, wi) * cos(wi) * 2 PI / N .
  For Importance sampling, we only sample rays that go towards light sources. If there are multiple light sources, we sample each
  light source NS_AREA times. The hit point is guaranteed to get the radiance from the light if there is no obstacles in the way. Using
  the Monte Carlo estimator, the one bounce radiance for a sample of a light is derived as bsdf(p,  wi->wo) * L(p, wi) * cos(wi) / pdf / N
  Finally, we summed up the radiance from all the lights.
</p>

<h3>
  Show some images rendered with both implementations of the direct lighting function.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <!-- Header -->
    <tr align="center">
      <th>
        <b>Uniform Hemisphere Sampling</b>
      </th>
      <th>
        <b>Light Sampling</b>
      </th>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/30.png" align="middle" width="400px"/>
        <figcaption>bunny.dae</figcaption>
      </td>
      <td>
        <img src="images/32.png" align="middle" width="400px"/>
        <figcaption>bunny.dae</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/31.png" align="middle" width="400px"/>
        <figcaption>spheres.dae</figcaption>
      </td>
      <td>
        <img src="images/33.png" align="middle" width="400px"/>
        <figcaption>spheres.dae</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>
<br>

<h3>
  Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b> when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, <b>not</b> uniform hemisphere sampling.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/34.png" align="middle" width="200px"/>
        <figcaption>1 Light Ray (dragon.dae)</figcaption>
      </td>
      <td>
        <img src="images/35.png" align="middle" width="200px"/>
        <figcaption>4 Light Rays (dragon.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/36.png" align="middle" width="200px"/>
        <figcaption>16 Light Rays (dragon.dae)</figcaption>
      </td>
      <td>
        <img src="images/37.png" align="middle" width="200px"/>
        <figcaption>64 Light Rays (dragon.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
  We notice that as the number of light rays used for sampling increases, the noise levels in the soft shadows decreases, with the shadow blending becoming smoother in terms of lighting and color.
  This makes sense, as with a higher ray count, the pixel values become more averaged out, and the monte carlo estimator becomes more accurate.
</p>
<br>

<h3>
  Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
</h3>
<p>
  We notice that the with uniform sampling, there is a lot of uniform noise distributed across the image caused by the randomness of light rays that sample in all directions in the hemisphere. With
  lighting sampling, the images look more natural and have more depth because the lighting direction is more pronounced due to our bias in sampling from light sources directly. Thus, it is easier
  to tell where light sources are coming from based on lighting gradients and shadows, resulting in a less "flat" and grainy picture. Additionally, this also means we use less computational resources to 
  get the same amount of light level, since most of the sampling done in uniform hemisphere sampling does not result in an intersection at all.
</p>
<br>


<h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
<!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

<h3>
  Walk through your implementation of the indirect lighting function.
</h3>
<p>
  We implemented the indirect lighting function to simulate bounced rays of light onto otherwise dark areas. In other words,
  this function allows non-light objects to also reflect light. To implement the indirect lighting function, we first derive
  the hit point (point where the ray intersects an object). Using
  one_bounce_radiance, we get the radiance from one bounce of light (direct lighting from part 3) for the luminosity for this single bounce. We sample the bsdf at the current
  hit point and get the pdf and incoming ray. For our base case of this recursive function, if the max_ray_depth is 1 or 0,
  we can simply return the one bounce radiance that we have calculated. Otherwise, we recursively perform a bounce with a probability
  of 0.7 (terminating probability of 0.3). If we decided to recurse (coinflip returns true), we then transform the incoming ray into world space with the o2w matrix and then create
  a new ray in the direction of the incoming ray with a decremented depth. After checking for an intersection of the new ray with another object (next bounce),
  we can call at_least_one_bounce_radiance recursively to obtain the next bounce's luminosity. We weigh this sample with the bsdf, cosine of
  incoming ray, and then normalize by the pdf and cpdf. Lastly, we add this to the accumulated L_out, which we then return.


</p>
<br>

<h3>
  Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/40.png" align="middle" width="400px"/>
        <figcaption>spheres.dae</figcaption>
      </td>
      <td>
        <img src="images/41.png" align="middle" width="400px"/>
        <figcaption>bunny.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/42.png" align="middle" width="400px"/>
        <figcaption>Only direct illumination (spheres.dae)</figcaption>
      </td>
      <td>
        <img src="images/43.png" align="middle" width="400px"/>
        <figcaption>Only indirect illumination (spheres.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
  It's interesting to see how the first image with only direct illumination shows the highlights on the spheres due to the first bounce of light
  . When only indirect lighting is used, the scene is very dark overall, as some of the light has already been reflected away by the first bounce, however we can still see color
  bleeding (red/blue on the spheres) since the next bounces may hit the wall then hit the sphere again.
</p>
<br>

<h3>
  For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/44.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/45.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/46.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/47.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/48a.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
  The case for m=0, m=1 is just zero-bounce and single-bounce respectively, and we notice some significant increase in brightness from m=1 to m=2, but the jump from m=2 to m=3 and EVEN m=3 to m=100 isn't as noticeable. We assume this is because most
  rays have already "dissipated" by now, future reflectiosn don't give off any significant luminosity anymore.
</p>
<br>

<h3>
  Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/48.png" align="middle" width="400px"/>
        <figcaption>1 sample per pixel (spheres.dae)</figcaption>
      </td>
      <td>
        <img src="images/49.png" align="middle" width="400px"/>
        <figcaption>2 samples per pixel (spheres.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/410.png" align="middle" width="400px"/>
        <figcaption>4 samples per pixel (spheres.dae)</figcaption>
      </td>
      <td>
        <img src="images/411.png" align="middle" width="400px"/>
        <figcaption>8 samples per pixel (spheres.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/412.png" align="middle" width="400px"/>
        <figcaption>16 samples per pixel (spheres.dae)</figcaption>
      </td>
      <td>
        <img src="images/413.png" align="middle" width="400px"/>
        <figcaption>64 samples per pixel (spheres.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/414.png" align="middle" width="400px"/>
        <figcaption>1024 samples per pixel (spheres.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
  We notice that when fewer samples are taken, there is much more noise. The noise is also chromatic and uniform in nature.
</p>
<br>


<h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
<!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

<h3>
  Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
</h3>
<p>
  Adaptive sampling helps us with efficiency of indirect lighting function because some bounces are not needed because some bounces
  don't significantly alter the illuminance. Hence, adaptive sampling terminates the sampling loop early if a value has converged, is no longer changing significantly enough (quantified by a tolerance). We implemented this by getting a
  running sum of the illumination and the illumination squared, which are then used to calculate the "convergence" wtih the formulas outlined in the spec. We also introduced a counter to count the number of samples.
</p>
<br>

<h3>
  Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/50.png" align="middle" width="400px"/>
        <figcaption>Rendered image (bunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/51.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (bunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/52.png" align="middle" width="400px"/>
        <figcaption>Rendered image (banana.dae)</figcaption>
      </td>
      <td>
        <img src="images/53.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (banana.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


</body>
</html>
